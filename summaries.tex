% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
 \geometry{margin=2cm} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options
\graphicspath{{figures/}} % NE FONCTIONNE PAS ??

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
%\usepackage{booktabs} % for much better looking tables
%\usepackage{array} % for better arrays (eg matrices) in maths
%\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
%\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{algorithmic}
\usepackage{algorithm}

%\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy
%\renewcommand{\headrulewidth}{0pt} % customise the layout...
%\lhead{}\chead{}\rhead{}
%\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
%\usepackage{sectsty}
%\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
%\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
%\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
%\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
%\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Summaries of a few papers on mitral valve prolapse and cross-modal image registration}
\author{Stephan}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\tableofcontents

%% RECHERCHER LE SCORE "DICE" UTILISÃ© POUR LA SEGMENTATION

%\section{Medical imaging problems}
%\begin{itemize}
%  \item Image registration
%  \begin{itemize}
%    \item Point cloud registration
%    \begin{itemize}
%      \item Iterative Closest Point algorithm \cite{besl:icp, chen:icp}
%      \item Random Sample Consensus (RanSaC) algorithm \cite{fischler:ransac}
%      \item Cross-modal point cloud registration \cite{huang:kinect}
%    \end{itemize}
%  
%    \item Intensity-based registration
%        \begin{itemize}
%           \item Electron microscopy to light microscopy \cite{acosta:intensity}
%        \end{itemize}
%    \item Cross-modal registration using image analogies
%        \begin{itemize}
%           \item Building dictionaries of features \cite{cao:analogies}
%        \end{itemize}
%  \end{itemize}
%  \item Analysis of cell images
%  \begin{itemize}
%    \item Segmentation
%    \begin{itemize}
%      \item Colour threshold and pixel adjacency
%      \item After segmentation, decision tree to classify an image segment as a cell or not a cell based on its area and elongation \cite{kan:machine}
%      \item Convolutional neural networks
%        \begin{itemize}
%           \item 2018 Data Science Bowl \cite{marks:bowl}
%        \end{itemize}
%    \end{itemize}
%    \item Tracking and constructing lineage trees
%        \begin{itemize}
%           \item Given two successive frames, classify all pairs (cell from first frame, cell from second frame) as "same cell" or "distinct cells" \cite{kan:machine}
%        \end{itemize}
%  \end{itemize}
%\end{itemize}


%  \item Decision tree to classify an image segment as a cell or not a cell based on its area and elongation;
%  \item Given two successive frames, classify all pairs (cell from first frame, cell from second frame) as "same cell" or "distinct cells";

\newpage

\section{Image registration}
  \subsection{ 3d Point set registration}
    \subsubsection{ Point set to point set: Iterative Closest Point algorithm \linebreak[4] \cite{besl:icp, chen:icp}}
An algorithm to register two sets of points. The basic principle is that it is easy to register two sets of points if the points are matched in pairs, and hard if they are not. See Algorithm \ref{algo:icp}.

\begin{algorithm}[h!]
\caption{Iterative Closest Point}
\label{algo:icp}
\begin{algorithmic}
  \REQUIRE two sets of points $X$ and $Y$; "find closest point" procedure; "find best transformation" procedure
  \ENSURE a transformation from $Y$ to $X$
  \REPEAT
    \FOR{every point $y \in Y$}
      \STATE find the point $x(y) \in X$ that is closest to $y$
    \ENDFOR
%    \STATE For every point $y \in Y$, find the point $x(y) \in X$ that is closest to $y$
    \STATE find the transformation $q$ that best maps all points in $Y$ to their image in $X$
    \STATE apply $q$ to $Y$
  \UNTIL{the transformation $q$ is smaller than some threshold, or the number of iterations is larger than some threshold }
  \RETURN the transformation obtained by composing all the successive transformations
\end{algorithmic}
\end{algorithm}

%\begin{itemize}
%  \item \emph{Input:} Two sets of points $X$ and $Y$; "find closest point" procedure; "find best transformation" procedure
%  \item Loop:
%  \begin{itemize}
%    \item For every point $y \in Y$, find the point $x(y) \in X$ that is closest to $y$
%    \item Find the transformation $q$ that best maps $Y$ to $\{ x(y),\, y \in Y \}$
%    \item Apply $q$ to $Y$
%  \end{itemize} 
%\end{itemize}

    \subsubsection{ Model to point set: Random Sample Consensus (RanSaC) algorithm \linebreak[4] \cite{fischler:ransac}}
%\cite{fischler:ransac}
The RanSaC algorithm: fit a model to a set of points. Assumes the model can be fitted using a small number of points, but can also fit "as best as possible" a larger number of points, for instance using least squares. The algorithm is robust to outliers. The algorithm is pretty general, and can be further adapted to a specific setting. It is random, but could possibly be made deterministic. See Algorithm \ref{algo:ransac}.

\begin{algorithm}[h!]
\caption{Random Sample Consensus}
\label{algo:ransac}
\begin{algorithmic}
  \REQUIRE a set of $N$ points; a model that requires $n < N$ points to instantiate; a tolerance threshold $\alpha$; a number of points threshold $t$
  \ENSURE an instance of the model
  \LOOP
    \STATE select $n$ points at random
    \STATE instantiate the model using these $n$ points
    \STATE find all points that fit the model within a tolerance error $\leq \alpha$
    \IF{the number of points is at least $t$}
      \STATE use all these points to instantiate a new model
      \RETURN the new model
    \ENDIF
  \ENDLOOP%{the transformation $q$ is smaller than some threshold, or the number of iterations is larger than some threshold }
  \RETURN failure; or alternatively, the model that fitted the highest number of points within $\alpha$
\end{algorithmic}
\end{algorithm}

%\clearpage

%\begin{itemize}
%  \item \emph{Input:} a set of $N$ points; a model that requires $n < N$ points to instantiate; a tolerance threshold $\alpha$; a number of points threshold $t$
%  \item Loop:
%  \begin{itemize}
%    \item Select $n$ points at random
%    \item Instantiate the model using these $n$ points
%    \item Find all points that fit the model with a tolerance error $\leq \alpha$
%    \item If the number of points is at least $t$:
%    \begin{itemize}
%      \item Use all these points to fit a new model
%      \item Terminate and return the new model
%    \end{itemize}
%    \item End if.
%  \end{itemize}
%  \item End loop.
%\end{itemize}

    \subsubsection{Cross-modal point cloud registration \cite{huang:kinect}}
Proposes an algorithm for registration of 3d clouds of points, with more than a thousand points: possibly tens of thousands or millions of points. The two clouds may have been generated using different kinds of sensors, and thus present differences, such as inequal point density or missing parts of the represented object: see Fig. \ref{fig:kinect}. The idea behind the algorithm is to extract the "macro" and "micro" structures from the images, and use them to build a graph for each image representing those structures. The point cloud registration problem becomes a graph matching problem. The structure extraction algorithm is adapted from a previous paper \cite{papon:supervoxels}.
See Algorithm \ref{algo:kinect}.

\begin{figure}
  \centering
  \includegraphics[width= \linewidth]{kinect.png}
  \caption{Difference in point density between Kinect (a) and a cloud reconstructed from multiple 2d RGB cameras (b). Illustration from \cite{huang:kinect}.}
  \label{fig:kinect}
\end{figure}

Mostly focused on point clouds obtained from these two sources: 1) Kinect; 2) reconstruction from multiple 2D RGB cameras. 
\begin{algorithm}
\caption{Point Cloud Registration \cite{huang:kinect}}
\label{algo:kinect}
\begin{algorithmic}
  \REQUIRE two 3d point clouds
  \ENSURE registration of the two clouds
  \STATE normalize the scales of the two images
  \STATE extract the macro and micro structures of each point cloud
  \STATE build a graph for each point cloud, representing its structure
  \STATE find best matching between the two graphs
  \STATE deduce the 3d transformation corresponding to the graph matching
  \RETURN the 3d transformation
\end{algorithmic}
\end{algorithm}
% Input images $\rightarrow$ Scale normalization $\rightarrow$ structure extraction $\rightarrow$ graph construction $\rightarrow$ optimization $\rightarrow$ transformation $\rightarrow$ result. See diagram p. 4. of that paper.
  
  \subsection{Intensity-based registration}
    \subsubsection{Using intensity for the similarity measure}
 Missing ref!!!

Only useful if all images come from the same modality. Usually requires intensity normalization of the two images.
    \subsubsection{Electron microscopy to light microscopy \cite{acosta:intensity}}
Intensity-based registration, specific for 3d light microscopy (LM) to 3d electron microscopy (EM). The 3d images are stacks; in particular, the size of a pixel in the $z$-axis differs from the size in the $x$ and $y$ axes.

First, a translation is computed to pre-align the images in the $xy$-plane, because the misalignment in that plane is expected to be greater than in the $z$-axis. A 3d region of interest is selected in the stack, by human intervention or by some other algorithm. The light microscopy stack is projected into a 2d image using maximum-intensity projection, while a few slices are selected from the electron microscopy stack. Then each EM slice is aligned to the LM projection, using a rotation-invariant similiarity measure on a histogram-based description of the Laplacian-of-Gaussian of each of the two images. The full LM stack is shifted by a weighted average of the translations corresponding to each EM slice.

Second, a 2d affine transformation is computed to align the 3d LoG-EM-RoI and the pre-aligned 3d LoG-LM-RoI. The LoG-LM stack is resampled with a low-pass filter and a bilinear interpolator, so that pixel size for the two images are the same. The transformation is computed using the similarity measure Mutual Information, and is computed in two steps: first, a rigid transformation, ie, a direct isometry; then, an additional affine transformation.

  \subsection{Cross-modal registration using image analogies}
    \subsubsection{Generating an image looking like modality $B$ given an image coming from modality $A$ \cite{hertzmann:analogies}}See Fig. \ref{fig:analogies}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.60 \linewidth]{analogies.png}
  \caption{After training on pairs of images coming from two different modalities, the neural network can simulate an image from the second modality when given an image from the first modality. Illustration from \cite{cao:analogies}.}
  \label{fig:analogies}
\end{figure}
    \subsubsection{Building dictionaries of features \cite{cao:analogies}}
Uses machine learning on a training dataset of pairs of images from two different modalities to learn two dictionaries $D_A$ and $D_B$ of small filters for each modality.

To register an image $I_A$ from modality $A$ with an image $I_B$ from modality $B$: first, the image $I_A$ from modality $A$ is converted into a sparse vector $\alpha$ such that $I_A$ can be reconstructed from vector $\alpha$ and dictionary $D_A$. Then an image $I_B'$ is constructed from $\alpha$ and $D_B$. Then images $I_B$ and $I_B'$ can be registered using a uni-modality registration algorithm. See Fig. \ref{fig:dico}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{dico.png}
  \caption{Summary of the registration algorithm using deep learning to extract dictionaries of features. Illustration from \cite{cao:analogies}.}
  \label{fig:dico}
\end{figure}


  \subsection{Using similarity measures with deep learning \cite{litjens:deeplearning}}
    \subsubsection{Extracting features in MR brain images with unsupervised deep learning \linebreak[4] \cite{wu:features} }
    \subsubsection{Extracting features for multi-modal registration with convolutional neural \linebreak[4] networks \cite{simonovsky:crossmodalfeatures}}
    \subsubsection{Extracting features for multi-modal registration with stacked auto-encoders \linebreak[4] \cite{cheng:deepsimilarity}}

\section{Analysis of cell images}
See Fig. \ref{fig:cells}.
\begin{figure}[h!]
  \centering
  \includegraphics[width= \linewidth]{cells.png}
  \caption{Common tasks in analysis of cell images. Illustration from \cite{kan:machine}.}
  \label{fig:cells}
\end{figure}
  \subsection{Cell segmentation}
    \subsubsection{Colour threshold and pixel adjacency}
    \subsubsection{After segmentation, decision tree to classify an image segment as a cell or not a cell based on its area and elongation \cite{kan:machine}}
    \subsubsection{Convolutional neural networks}
       \paragraph{2018 Data Science Bowl \cite{marks:bowl}}
A contest on kaggle to devise algorithms for cell segmentation. The algorithms are required to be able to handle images from several different experiments using light microscopy. The three best ranked submissions to the contest use convolutional neural networks and perform significantly better than the software CellProfiler, while not requiring the user to spend time configuring the software. The good performance of these solutions appears to come in particular from the succesful use of data augmentation and data preprocessing. See Fig. \ref{fig:bowl}.
\begin{figure}[h!]
  \centering
  \includegraphics[width= \linewidth ]{bowl.png}
  \caption{Illustration from \cite{marks:bowl}.}
  \label{fig:bowl}
\end{figure}



  \subsection{Tracking and constructing lineage trees}
    \subsubsection{Classification problem: Given two successive frames, classify all pairs (cell from first frame, cell from second frame) as "same cell" or "distinct cells" \cite{kan:machine}}
\begin{figure}[h!]
  \centering
  \includegraphics[width=.75 \linewidth ]{tracking.png}
  \caption{Illustration from \cite{kan:machine}.}
  \label{fig:tracking}
\end{figure}
See Fig. \ref{fig:tracking}.




\section{Other applications of deep learning in medical imaging \cite{litjens:deeplearning}}
  \subsection{Classification of images from medical exams}
  \subsection{Content-based image retrieval}
    Browsing massive databases, for instance retrieving similar case histories. Feature extraction is very useful here.
  \subsection{Generating text reports from images \cite{schlegl:text}}











%
%
%\section{Crocoval}
%
%\section{ICP
%\cite{besl:icp, chen:icp}}
%Presents the Iterative Closest Point algorithm, to register two 3d shapes.
%
%\section{Random sample consensus
%\cite{fischler:ransac}}
%Presents the RanSaC algorithm.
%\begin{itemize}
%  \item \emph{Input:} a set of $N$ points; a model that requires $n < N$ points to instantiate; a tolerance threshold $\alpha$; a number of points threshold $t$
%  \item Loop:
%  \begin{itemize}
%    \item Select $n$ points at random
%    \item Instantiate the model using these $n$ points
%    \item Find all points that fit the model with a tolerance error $\leq \alpha$
%    \item If the number of points is at least $t$:
%    \begin{itemize}
%      \item Use all these points to fit a new model
%      \item Terminate and return the new model
%    \end{itemize}
%    \item End if.
%  \end{itemize}
%  \item End loop.
%\end{itemize}
%
%
%\section{Point cloud registration
%\cite{huang:kinect}}
%
%
%Proposes an algorithm for registration of 3d clouds of points, with more than a thousand points: possibly tens of thousands or millions of points. The two clouds may have been generated using different kinds of sensors, and thus present differences, such as inequal point density or missing parts of the represented object. The idea behind the algorithm is to extract the "macro" and "micro" structures from the images, and use them to build a graph for each image representing those structures. The point cloud registration problem becomes a graph matching problem. The structure extraction algorithm is adapted from a previous paper \cite{papon:supervoxels}.
%
% Input images $\rightarrow$ Scale normalization $\rightarrow$ structure extraction $\rightarrow$ graph construction $\rightarrow$ optimization $\rightarrow$ transformation $\rightarrow$ result. See diagram p. 4. of that paper.
%
%Mostly focused on point clouds obtained from these two sources: 1) Kinect; 2) reconstruction from multiple 2D RGB cameras. 
%
%\section{Registration using image analogies 
%\cite{cao:analogies}}
%
%Uses machine learning on a training dataset of pairs of images from two different modalities to learn two dictionaries $D_A$ and $D_B$ of small filters for each modality.
%
%To register an image $I_A$ from modality $A$ with an image $I_B$ from modality $B$: first, the image $I_A$ from modality $A$ is converted into a sparse vector $\alpha$ such that $I_A$ can be reconstructed from vector $\alpha$ and dictionary $D_A$. Then an image $I_B'$ is constructed from $\alpha$ and $D_B$. Then images $I_B$ and $I_B'$ can be registered using a uni-modality registration algorithm. See Fig. 3 p. 4 in the paper.
%
%
%\section{Future of CLEM
%\cite{perrine:future}}
%
%
%\section{Intensity-based matching
%\cite{acosta:intensity}}
%
%Intensity-based registration, specific for 3d light microscopy to 3d electron microscopy. The 3d images are stacks; in particular, the size of a pixel in the Z-axis differs from the size in the X and Y axes.
%
%First, the images pre-aligned in the XY-plane, because the misalignment in that plane is expected to be greater than in the Z-axis. A 3d region of interest is selected in the stack, by human intervention or by some other algorithm. The light microscopy stack is projected into a 2D image using maximum-intensity projection, while a few slices are selected from the electron microscopy stack. Then each EM slice is aligned to the LM projection, using a rotation-invariant similiarity measure on a histogram-based description of the Laplacian-of-Gaussian of each of the two images. The full LM stack is shifted by a weighted average of the translations corresponding to each EM slice.
%
%Second, a 2d affine transformation is computed to align the 3d LoG-EM-RoI and the pre-aligned 3d LoG-LM-RoI. The LoG-LM stack is resampled with a low-pass filter and a bilinear interpolator, so that pixel size for the two images are the same. The transformation is computed using the similarity measure Mutual Information, and is computed in two steps: first, a rigid transformation, ie, a direct isometry; then, an additional affine transformation.
%
%
%\section{Multi-modal registration 2d 3d
%\cite{acosta:these}}
%
%
%\section{Machine-learning and cells
%\cite{kan:machine}}
%A brief survey on applications of machine learning to the analysis of images of cells acquired with light microscopy, for instance segmenting, tracking, constructing lineage trees.
%%\begin{itemize}
%%  \item Decision tree to classify an image segment as a cell or not a cell based on its area and elongation;
%%  \item Given two successive frames, classify all pairs (cell from first frame, cell from second frame) as "same cell" or "distinct cells";
%%\end{itemize}
%
%
%
%\section{Data Science Bowl
%\cite{marks:bowl}}
%
%A contest on kaggle to devise algorithms for cell segmentation. The algorithms are required to be able to handle images from several different experiments using light microscopy. The three best ranked submissions to the contest use convolutional neural networks and perform significantly better than the software CellProfiler, while not requiring the user to spend time configuring the software. The good performance of these solutions appears to come in particular from the succesful use of data augmentation and data preprocessing.
%
%
%\section{New insights on mitral valve: Filamin-A
%\cite{letourneau:filamina}}


\bibliographystyle{apalike}
\bibliography{summaries}

\end{document}
